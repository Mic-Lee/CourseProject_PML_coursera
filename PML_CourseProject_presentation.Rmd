---
title: "PML_CourseProject_present"
author: "Haoming"
date: "3/24/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r}
library(caret)
library(data.table)
library(rpart)
library(rpart.plot)
library(RColorBrewer)
library(rattle)
library(randomForest)
library(knitr)
library(e1071)

#loading data
path <- getwd()
training.path <- file.path(path, "pml-training.csv")
testing.path <- file.path(path,"pml-testing.csv")
training.data <- fread(training.path)
testing.data <- fread(testing.path)

#processing data
## sub.train and sub.test
inTrain <- createDataPartition(y=training.data$classe, p=0.6, list=FALSE)
myTraining <- training.data[inTrain, ]
myTesting <- training.data[-inTrain, ]
dim(myTraining); dim(myTesting)
##transfer inorder to clean
myTraining <- as.data.frame(myTraining)
myTesting <- as.data.frame(myTesting)
##clean var with more than 55% NAs
training.minusNA <- myTraining 
for(i in 1:length(myTraining)) { 
  if( sum( is.na( myTraining[, i] ) ) /nrow(myTraining) >= .6 ) { 
    for(j in 1:length(training.minusNA)) {
      if( length( grep(names(myTraining[i]), names(training.minusNA)[j]) ) ==1)  { 
        training.minusNA <- training.minusNA[ , -j] 
      }   
    } 
  }
}
myTraining <- training.minusNA
rm(training.minusNA)
##remove some personal and nonrelative vars
myTraining <- myTraining[,8:length(myTraining)]
##remove NearZeroVariance variables
nzv.tr <- nearZeroVar(myTraining, saveMetrics=TRUE)
myTraining <- myTraining[,nzv.tr$nzv==FALSE]


##synchronizing processing
myTesting <- myTesting[colnames(myTraining)]
testing.data <- as.data.frame(testing.data)
testing.data <- testing.data[colnames(myTraining[, -53])]             
dim(myTesting);dim(testing.data)

##coerce the data into the same types
for (i in 1:length(testing.data) ) {
  for(j in 1:length(myTraining)) {
    if( length( grep(names(myTraining[i]), names(testing.data)[j]) ) == 1)  {
      class(testing.data[j]) <- class(myTraining[i])
    }      
  }      
}

##for random forest
myTraining$classe = as.factor(myTraining$classe)
myTesting$classe = as.factor(myTesting$classe)

```

```{r}
#pre with decision tree
set.seed(12345)
modFitA1 <- rpart(classe ~ ., data=myTraining, method="class")
fancyRpartPlot(modFitA1, tweak = 2)
# rpart.plot(modFitA1)
# prp(modFitA1, main="An Example",
#     type=1, fallen=T, branch=.3, round=0, leaf.round=9,
#     clip.right.labs=F, under.cex=1,
#     box.palette="GnYlRd",
#     prefix="ozone\n", branch.col="gray", branch.lwd=2,
#     extra=101, under=T, lt=" < ", ge=" >= ", cex.main=1.5,tweak=1.2,gap = 0)

predictionsA1 <- predict(modFitA1, myTesting, type = "class")
cmtree <- confusionMatrix(predictionsA1, myTesting$classe)
cmtree
plot(cmtree$table, col = cmtree$byClass, main = paste("Decision Tree Confusion Matrix: Accuracy =", round(cmtree$overall['Accuracy'], 4)))

#pre with random forest
set.seed(12345)

modFitB1 <- randomForest(classe ~ ., data=myTraining, method = "Class")
predictionB1 <- predict(modFitB1, myTesting, type = "class")
cmrf <- confusionMatrix(predictionB1, myTesting$classe)
cmrf
plot(modFitB1)
plot(cmrf$table, col = cmtree$byClass, main = paste("Random Forest Confusion Matrix: Accuracy =", round(cmrf$overall['Accuracy'], 4)))#this plot referenced from online resourse

#pre with SVM
# linear SVM
svmfit <- svm(classe ~ ., data = myTraining, kernel = "linear", scale = T) # linear svm, scaling turned OFF
print(svmfit)
compareTable <- table (myTraining$classe, predict(svmfit));compareTable  # tabulate
mean(myTraining$classe != predict(svmfit)) # 20.66% misclassification error

#then the random forest model is the best model
bestfitmodel <- modFitB1

#Predicting Results on the Test Data
predictionB2 <- predict(bestfitmodel, testing.data, type = "class")
predictionB2
```

